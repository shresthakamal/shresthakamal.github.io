<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Creating RAG application using Langgraph | Kamal  Shrestha (कमल श्रेष्ठ)</title>
    <meta name="author" content="Kamal  Shrestha (कमल श्रेष्ठ)">
    <meta name="description" content="stateful langgraph nodes for RAG applications rather than multiple tools selection in LCEL">
    <meta name="keywords" content="natural-language-processing, large-language-models, machine-learning, deep-learning, recommendation-system, nlp, academic-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://shresthakamal.com.np/blog/2024/code/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kamal </span>Shrestha (कमल श्रेष्ठ)</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about</a>
          </li>

          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">blogs<span class="sr-only">(current)</span></a>
          </li>

          <!-- 
              <li class="nav-item ">
                <a class="nav-link" href="https://shresthakamal.com.np/photography/">gallery</a>
              </li> -->


          <!-- Other pages -->
          <li class="nav-item ">

            <a class="nav-link" href="/publications/">publications</a>
          </li>
          <li class="nav-item ">

            <a class="nav-link" href="/projects/">projects</a>
          </li>
          <li class="nav-item ">

            <a class="nav-link" href="/experience/">experience</a>
          </li>
          <li class="nav-item ">

            <a class="nav-link" href="/photography/">gallery</a>
          </li>
          <!-- Toogle theme mode -->
          <li class="toggle-container">
            <button id="light-toggle" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Creating RAG application using Langgraph</h1>
    <p class="post-meta">March 1, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/rag">
          <i class="fas fa-hashtag fa-sm"></i> rag</a>  
          <a href="/blog/tag/nlp">
          <i class="fas fa-hashtag fa-sm"></i> nlp</a>  
          <a href="/blog/tag/langchain">
          <i class="fas fa-hashtag fa-sm"></i> langchain</a>  
          <a href="/blog/tag/llm">
          <i class="fas fa-hashtag fa-sm"></i> llm</a>  
          
        ·  
        <a href="/blog/category/nlp">
          <i class="fas fa-tag fa-sm"></i> nlp</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h2 id="prerequisites">Prerequisites</h2>
<ol>
  <li>Good Knowledge on langchain, creating agents used for multiple tools selections</li>
  <li>Well-versed in retrieval-augmented generations, vector databases, and text embeddings</li>
</ol>

<h2 id="learning-objectives">Learning Objectives</h2>
<p>By the end of this post, all readers will be able to:</p>
<ol>
  <li>Understand the difference between langchain agents with multiple tools selections and langgraphs</li>
  <li>Create a langgraph agentic rag using stateful nodes capable of making logical decisions using LLMs</li>
</ol>

<h2 id="langchain-vs-langgraph">Langchain Vs Langgraph</h2>

<p style="text-align:justify;">
<img src="/assets/blogs/langgraph-agentic-rag/langgraph.webp" style="float: right;padding:20px">

Langchain is constructed upon the fundamental principle of a sequential acyclic flow of execution, spanning from user inquiry to the generation of results by the Language Model (LLM). 
<br>
In RAG (Retrieval-Augmented Generation), the procedural flow within the application starts with the user query, progresses through stages such as database extraction, tool selection, context enrichment, and ultimately finishes at result generation. Furthermore, this process may also encompass evaluation, query transformations, and similar steps. The essence of Langchain lies in its utilization of interconnected "chains," wherein each element is systematically linked in a sequential fashion. 
<br><br>
It's important to understand that the decision-making process in this context operates sequentially. When choices are made, they set the course for subsequent actions, creating a flow of information. What's significant about this approach is the absence of a feedback loop or the opportunity for re-evaluation based on outcomes. Among its various capabilities, langgraph is specifically designed to introduce a cyclic, stateful, multi-actor application framework using LLMs. Previously, Langchain agents could only re-execute a single action using CoT prompts like ReAct. They could assess the outcome, observe results, and potentially adjust course. 
<br><br>
However, the ability to reprocess multiple steps of execution was unavailable until the introduction of langgraph. With langgraph, we now have the capability to re-execute multiple elements in a feedback loop fashion, enabling a more iterative approach to decision-making and action.
<br>
Agents developed within langgraph will possess the capability to make decisions, assess their effectiveness, and if necessary, rerun processes to enhance output. This functionality extends the LangChain Expression Language by facilitating the coordination of multiple chains or actors across various stages of computation in a cyclic fashion, as stated by LangChain. With langgraph, you now have the flexibility to outline an application resembling a flowchart, with each decision node rooted in the logical determinations of LLMs. This enables a structured and iterative approach to application development, allowing for refinement and optimization at every step.
<br><br><br>

<b>Now, in this blog post, we will be creating a RAG agent using langgraph by designing a sequence of execution nodes and the overall flow of the application.</b>
</p>

<p><br></p>

<h2 id="rag-using-agents-in-langgraph">RAG using agents in langgraph</h2>

<p><br></p>

<h3 id="installations">Installations</h3>
<p>The first and foremost thing to do is to make sure all the necessary libraries are installed. Make sure you have the upgraded version of langchain because a lot of modules are segregated out of langchain to openai, community and hub.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
</pre></td>
<td class="code"><pre>    <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_community</span> <span class="n">langchain</span><span class="o">-</span><span class="n">openai</span> <span class="n">langchainhub</span> <span class="n">chromadb</span> <span class="n">langchain</span> <span class="n">langgraph</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="load-openai-api-keys">Load OPENAI API keys</h3>
<p>We will use the python-dotenv package to load our api keys.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td>
<td class="code"><pre>    <span class="k">import</span> <span class="n">os</span>

    <span class="n">from</span> <span class="n">dotenv</span> <span class="k">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

    <span class="cp"># read local .env file
</span>    <span class="n">_</span> <span class="o">=</span> <span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="indexing">Indexing</h3>

<p>The initial step in any machine learning projects is to collect the data and conduct pre processing for further usage.</p>

<ol>
  <li>In this step here, we will be loading our data from three blog posts from Lilian Weng using the <code class="language-plaintext highlighter-rouge">WebBaseLoader</code> from langchain that loads the data in the form of txt files in a list.</li>
  <li>Once each links are processed in a list of <code class="language-plaintext highlighter-rouge">Document</code> objects in langchain, we will start creating the chunks.</li>
  <li>We will use <code class="language-plaintext highlighter-rouge">RecursiveCharacterTextSplitter</code> with <code class="language-plaintext highlighter-rouge">chunk_size = 100</code> and <code class="language-plaintext highlighter-rouge">chunk_overlap = 50</code> to create document chunks</li>
  <li>Creating a vector store using <code class="language-plaintext highlighter-rouge">chromadb</code>.</li>
</ol>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td>
<td class="code"><pre>    <span class="cp"># necessary imports
</span>    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">text_splitter</span> <span class="k">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
    <span class="n">from</span> <span class="n">langchain_community</span><span class="p">.</span><span class="n">document_loaders</span> <span class="k">import</span> <span class="n">WebBaseLoader</span>
    <span class="n">from</span> <span class="n">langchain_community</span><span class="p">.</span><span class="n">vectorstores</span> <span class="k">import</span> <span class="n">Chroma</span>
    <span class="n">from</span> <span class="n">langchain_openai</span> <span class="k">import</span> <span class="n">OpenAIEmbeddings</span>

    <span class="cp"># data sources
</span>    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-06-23-agent/"</span><span class="p">,</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"</span><span class="p">,</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="cp"># get the documents in txt based formats
</span>    <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">).</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="n">in</span> <span class="n">urls</span><span class="p">]</span>
    <span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="n">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="n">in</span> <span class="n">sublist</span><span class="p">]</span>

    <span class="cp"># split the documents into chunks
</span>    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span>
    <span class="p">)</span>
    <span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>
    
    <span class="cp"># create a chromadb based on the chunks above
</span>    <span class="cp"># each chunks are embedded using OPENAI embeddings and the database is named as "rag-chroma"
</span>    <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="n">from_documents</span><span class="p">(</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s">"rag-chroma"</span><span class="p">,</span>
        <span class="n">embedding</span><span class="o">=</span><span class="n">OpenAIEmbeddings</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="cp"># instantiate a vector store backed retriever
</span>    <span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="n">as_retriever</span><span class="p">()</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<p>We will now create an agent tool that the agent can access while retreiving documents from the chroma database. This is very similar to what we do in LCEL / langchain while creating a tool and a tool executor.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td class="code"><pre>    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">tools</span><span class="p">.</span><span class="n">retriever</span> <span class="k">import</span> <span class="n">create_retriever_tool</span>

    <span class="cp"># Tool Definition with the name of the tool and its description
</span>    <span class="n">tool</span> <span class="o">=</span> <span class="n">create_retriever_tool</span><span class="p">(</span>
        <span class="n">retriever</span><span class="p">,</span>
        <span class="s">"retrieve_blog_posts"</span><span class="p">,</span>
        <span class="s">"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs."</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>

    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">prebuilt</span> <span class="k">import</span> <span class="n">ToolExecutor</span>
    <span class="n">tool_executor</span> <span class="o">=</span> <span class="n">ToolExecutor</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
    <span class="cp"># tool_executor that can be called with agent</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="states-in-a-langgraph">States in a langgraph</h3>

<p>A state in a langgraph is very similar to attributes in a class object. Whenever a flow execution moves from one node to another node, the output of nodes are written in the states object (overridden or added). Overridden means change in state whereas  addition means collection of states. For example: chat messages will be added where as grades of documents will be overidden.
<br></p>

<p>You can think of states as sessions values as well like in streamlit (if you have used streamlit) or cache.
<br></p>

<p>In the following example, we define a <code class="language-plaintext highlighter-rouge">AgentState</code> class that collects all the messages generated in the execution. It is nothing but a dictionary with key, value pairs.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td class="code"><pre>    <span class="k">import</span> <span class="k">operator</span>
    <span class="n">from</span> <span class="n">typing</span> <span class="k">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">TypedDict</span>

    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">BaseMessage</span>


    <span class="k">class</span> <span class="nc">AgentState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">)</span><span class="o">:</span>
        <span class="n">messages</span><span class="o">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="k">operator</span><span class="p">.</span><span class="n">add</span><span class="p">]</span>

    <span class="cp"># Here, the opetor.add is defined so that we can simply pass the Sequence[BaseMessage] as parameters
</span>    <span class="cp"># and no need to define add functions. like AgentState([k, v])</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="nodes-of-execution">Nodes of execution</h3>
<p><img src="/assets/blogs/langgraph-agentic-rag/state-flow.png" width="100%" align="center"></p>

<h4 id="imports">imports</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td class="code"><pre>    <span class="n">from</span> <span class="n">langchain</span> <span class="k">import</span> <span class="n">hub</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">output_parsers</span> <span class="k">import</span> <span class="n">PydanticOutputParser</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">prompts</span> <span class="k">import</span> <span class="n">PromptTemplate</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">tools</span><span class="p">.</span><span class="n">render</span> <span class="k">import</span> <span class="n">format_tool_to_openai_function</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">function_calling</span> <span class="k">import</span> <span class="n">convert_to_openai_tool</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">BaseMessage</span><span class="p">,</span> <span class="n">FunctionMessage</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">output_parsers</span><span class="p">.</span><span class="n">openai_tools</span> <span class="k">import</span> <span class="n">PydanticToolsParser</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">pydantic_v1</span> <span class="k">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
    <span class="n">from</span> <span class="n">langchain_openai</span> <span class="k">import</span> <span class="n">ChatOpenAI</span>
    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">prebuilt</span> <span class="k">import</span> <span class="n">ToolInvocation</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">output_parsers</span> <span class="k">import</span> <span class="n">StrOutputParser</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="agent-node">agent node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">agent</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Invokes the agent model to generate a response based on the current state. Given</span><span class="err">
</span><span class="s">        the question, it will decide to retrieve using the retriever tool, or simply end.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with the agent response apended to messages</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---CALL AGENT---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">format_tool_to_openai_function</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="n">in</span> <span class="n">tools</span><span class="p">]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">bind_functions</span><span class="p">(</span><span class="n">functions</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="cp"># We return a list, because this will get added to the existing list
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="retreive-node">retreive node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Uses tool to execute retrieval.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with retrieved docs</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---EXECUTE RETRIEVAL---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="cp"># Based on the continue condition
</span>        <span class="cp"># we know the last message involves a function call
</span>        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="cp"># We construct an ToolInvocation from the function_call
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">ToolInvocation</span><span class="p">(</span>
            <span class="n">tool</span><span class="o">=</span><span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s">"function_call"</span><span class="p">][</span><span class="s">"name"</span><span class="p">],</span>
            <span class="n">tool_input</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span>
                <span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s">"function_call"</span><span class="p">][</span><span class="s">"arguments"</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="cp"># We call the tool_executor and get back a response
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">tool_executor</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">function_message</span> <span class="o">=</span> <span class="n">FunctionMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">str</span><span class="p">(</span><span class="n">response</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">action</span><span class="p">.</span><span class="n">tool</span><span class="p">)</span>

        <span class="cp"># We return a list, because this will get added to the existing list
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">function_message</span><span class="p">]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="should_retreive-node">should_retreive node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">should_retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Decides whether the agent should retrieve more information or end the process.</span><span class="err">

</span><span class="s">        This function checks the last message in the state for a function call. If a function call is</span><span class="err">
</span><span class="s">        present, the process continues to retrieve information. Otherwise, it ends the process.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            str: A decision to either "</span><span class="k">continue</span><span class="s">" the retrieval process or "</span><span class="n">end</span><span class="s">" it</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---DECIDE TO RETRIEVE---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="cp"># If there is no function call, then we finish
</span>        <span class="k">if</span> <span class="s">"function_call"</span> <span class="n">not</span> <span class="n">in</span> <span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DO NOT RETRIEVE / DONE---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"end"</span>
        <span class="cp"># Otherwise there is a function call, so we continue
</span>        <span class="k">else</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: RETRIEVE---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"continue"</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="grade_documents-node">grade_documents node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">grade_documents</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Determines whether the retrieved documents are relevant to the question.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            str: A decision for whether the documents are relevant or not</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---CHECK RELEVANCE---"</span><span class="p">)</span>

        <span class="cp"># Data model
</span>        <span class="k">class</span> <span class="nc">grade</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">)</span><span class="o">:</span>
            <span class="s">"""Binary score for relevance check."""</span>

            <span class="n">binary_score</span><span class="o">:</span> <span class="n">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Relevance score 'yes' or 'no'"</span><span class="p">)</span>

        <span class="cp"># LLM
</span>        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="cp"># Tool
</span>        <span class="n">grade_tool_oai</span> <span class="o">=</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">grade</span><span class="p">)</span>

        <span class="cp"># LLM with tool and enforce invocation
</span>        <span class="n">llm_with_tool</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">bind</span><span class="p">(</span>
            <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">grade_tool_oai</span><span class="p">)],</span>
            <span class="n">tool_choice</span><span class="o">=</span><span class="p">{</span><span class="s">"type"</span><span class="o">:</span> <span class="s">"function"</span><span class="p">,</span> <span class="s">"function"</span><span class="o">:</span> <span class="p">{</span><span class="s">"name"</span><span class="o">:</span> <span class="s">"grade"</span><span class="p">}},</span>
        <span class="p">)</span>

        <span class="cp"># Parser
</span>        <span class="n">parser_tool</span> <span class="o">=</span> <span class="n">PydanticToolsParser</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">grade</span><span class="p">])</span>

        <span class="cp"># Prompt
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
            <span class="k">template</span><span class="o">=</span><span class="s">"""You are a grader assessing relevance of a retrieved document to a user question. </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">            Here is the retrieved document: </span><span class="se">\n\n</span><span class="s"> {context} </span><span class="se">\n\n</span><span class="err">
</span><span class="s">            Here is the user question: {question} </span><span class="se">\n</span><span class="err">
</span><span class="s">            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. </span><span class="se">\n</span><span class="err">
</span><span class="s">            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""</span><span class="p">,</span>
            <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s">"context"</span><span class="p">,</span> <span class="s">"question"</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="cp"># Chain
</span>        <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm_with_tool</span> <span class="o">|</span> <span class="n">parser_tool</span>

        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="p">.</span><span class="n">content</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="o">:</span> <span class="n">question</span><span class="p">,</span> <span class="s">"context"</span><span class="o">:</span> <span class="n">docs</span><span class="p">})</span>

        <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">binary_score</span>

        <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s">"yes"</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DOCS RELEVANT---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"yes"</span>

        <span class="nl">else:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DOCS NOT RELEVANT---"</span><span class="p">)</span>
            <span class="n">print</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">binary_score</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"no"</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="generate-node">generate node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">generate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Generate answer</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with re-phrased question</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---GENERATE---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="p">.</span><span class="n">content</span>

        <span class="cp"># Prompt
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="n">pull</span><span class="p">(</span><span class="s">"rlm/rag-prompt"</span><span class="p">)</span>

        <span class="cp"># LLM
</span>        <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="cp"># Post-processing
</span>        <span class="n">def</span> <span class="n">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">:</span>
            <span class="k">return</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="n">in</span> <span class="n">docs</span><span class="p">)</span>

        <span class="cp"># Chain
</span>        <span class="n">rag_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

        <span class="cp"># Run
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"context"</span><span class="o">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s">"question"</span><span class="o">:</span> <span class="n">question</span><span class="p">})</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h4 id="rewrite-node">rewrite node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td>
<td class="code"><pre>    <span class="n">def</span> <span class="n">rewrite</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Transform the query to produce a better question.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with re-phrased question</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---TRANSFORM QUERY---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">HumanMessage</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">f</span><span class="s">""" </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">        Look at the input and try to reason about the underlying semantic intent / meaning. </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">        Here is the initial question:</span><span class="err">
</span><span class="s">        </span><span class="se">\n</span><span class="s"> ------- </span><span class="se">\n</span><span class="err">
</span><span class="s">        {question} </span><span class="err">
</span><span class="s">        </span><span class="se">\n</span><span class="s"> ------- </span><span class="se">\n</span><span class="err">
</span><span class="s">        Formulate an improved question: """</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="cp"># Grader
</span>        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="graph--workflow">Graph / Workflow</h3>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td>
<td class="code"><pre>    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">graph</span> <span class="k">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span>

    <span class="cp"># Define a new graph
</span>    <span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">AgentState</span><span class="p">)</span>

    <span class="cp"># Define the nodes we will cycle between
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"agent"</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>  <span class="err">#</span> <span class="n">agent</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"retrieve"</span><span class="p">,</span> <span class="n">retrieve</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"rewrite"</span><span class="p">,</span> <span class="n">rewrite</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"generate"</span><span class="p">,</span> <span class="n">generate</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>


    <span class="cp"># Call agent node to decide to retrieve or not
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">set_entry_point</span><span class="p">(</span><span class="s">"agent"</span><span class="p">)</span>

    <span class="cp"># Decide whether to retrieve
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
        <span class="s">"agent"</span><span class="p">,</span>
        <span class="cp"># Assess agent decision
</span>        <span class="n">should_retrieve</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="cp"># Call tool node
</span>            <span class="s">"continue"</span><span class="o">:</span> <span class="s">"retrieve"</span><span class="p">,</span>
            <span class="s">"end"</span><span class="o">:</span> <span class="n">END</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="cp"># Edges taken after the `action` node is called.
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
        <span class="s">"retrieve"</span><span class="p">,</span>
        <span class="cp"># Assess agent decision
</span>        <span class="n">grade_documents</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="s">"yes"</span><span class="o">:</span> <span class="s">"generate"</span><span class="p">,</span>
            <span class="s">"no"</span><span class="o">:</span> <span class="s">"rewrite"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s">"generate"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s">"rewrite"</span><span class="p">,</span> <span class="s">"agent"</span><span class="p">)</span>

    <span class="cp"># Compile
</span>    <span class="n">app</span> <span class="o">=</span> <span class="n">workflow</span><span class="p">.</span><span class="n">compile</span><span class="p">()</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<h3 id="user-input-and-generation">User Input and Generation</h3>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td>
<td class="code"><pre>    <span class="k">import</span> <span class="n">pprint</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">HumanMessage</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Tell me about langgraph"</span><span class="p">)]}</span>
    <span class="k">for</span> <span class="n">output</span> <span class="n">in</span> <span class="n">app</span><span class="p">.</span><span class="n">stream</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="n">in</span> <span class="n">output</span><span class="p">.</span><span class="n">items</span><span class="p">()</span><span class="o">:</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">f</span><span class="s">"Output from node '{key}':"</span><span class="p">)</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="s">"---"</span><span class="p">)</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">None</span><span class="p">)</span>
        <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">---</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"langgraph"}', 'name': 'retrieve_blog_posts'}})]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: RETRIEVE---
---EXECUTE RETRIEVAL---
"Output from node 'retrieve':"
'---'
{ 'messages': [ FunctionMessage(content='$$\n\n$$\n\n$$\n\n$$', name='retrieve_blog_posts')]}
'\n---\n'
---CHECK RELEVANCE---
---DECISION: DOCS NOT RELEVANT---
no
---TRANSFORM QUERY---
"Output from node 'rewrite':"
'---'
{ 'messages': [ AIMessage(content='What is the purpose or function of langgraph?')]}
'\n---\n'
---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='I couldn\'t find specific information about "langgraph" in Lilian Weng\'s blog posts. Could you provide more context or details about langgraph so I can better assist you?')]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: DO NOT RETRIEVE / DONE---
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Tell me about langgraph'),
                AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"langgraph"}', 'name': 'retrieve_blog_posts'}}),
                FunctionMessage(content='$$\n\n$$\n\n$$\n\n$$', name='retrieve_blog_posts'),
                AIMessage(content='What is the purpose or function of langgraph?'),
                AIMessage(content='I couldn\'t find specific information about "langgraph" in Lilian Weng\'s blog posts. Could you provide more context or details about langgraph so I can better assist you?')]}
'\n---\n'
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
</pre></td>
<td class="code"><pre>    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Differentiate between agents and chain in langchain"</span><span class="p">)]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OUTPUT

---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='In Langchain, an "agent" refers to an entity that interacts with the Langchain network to perform various tasks, such as executing smart contracts, processing transactions, or providing services. Agents in Langchain can be autonomous entities that act on behalf of users or organizations to carry out specific functions within the network.\n\nOn the other hand, a "chain" in Langchain typically refers to the blockchain network itself. It is the decentralized and distributed ledger that stores transaction data, smart contracts, and other information in a secure and immutable manner. The chain in Langchain is responsible for maintaining the integrity of the network, validating transactions, and ensuring consensus among network participants.\n\nIn summary, agents are the entities that interact with the Langchain network to perform tasks, while the chain represents the underlying blockchain infrastructure that facilitates these interactions and maintains the network\'s operations.')]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: DO NOT RETRIEVE / DONE---
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Differentiate between agents and chain in langchain'),
                AIMessage(content='In Langchain, an "agent" refers to an entity that interacts with the Langchain network to perform various tasks, such as executing smart contracts, processing transactions, or providing services. Agents in Langchain can be autonomous entities that act on behalf of users or organizations to carry out specific functions within the network.\n\nOn the other hand, a "chain" in Langchain typically refers to the blockchain network itself. It is the decentralized and distributed ledger that stores transaction data, smart contracts, and other information in a secure and immutable manner. The chain in Langchain is responsible for maintaining the integrity of the network, validating transactions, and ensuring consensus among network participants.\n\nIn summary, agents are the entities that interact with the Langchain network to perform tasks, while the chain represents the underlying blockchain infrastructure that facilitates these interactions and maintains the network\'s operations.')]}
'\n---\n'
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
</pre></td>
<td class="code"><pre>    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Differentiate zero shot and few shot prompting from the blog posts"</span><span class="p">)]}</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<pre><code class="language-OUTPUT">
---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"zero shot prompting vs few shot prompting"}', 'name': 'retrieve_blog_posts'}})]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: RETRIEVE---
---EXECUTE RETRIEVAL---
"Output from node 'retrieve':"
'---'
{ 'messages': [ FunctionMessage(content="Basic Prompting#\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\nZero-Shot#\nZero-shot learning is to simply feed the task text to the model and ask for results.\n(All the sentiment analysis examples are from SST-2)\nText: i'll bet the video game is a lot more fun than the film.\n\nInstruction Prompting#\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\propto \\exp(r(\\mathbf{x}, \\mathbf{y}) / \\tau)$\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.", name='retrieve_blog_posts')]}
'\n---\n'
---CHECK RELEVANCE---
---DECISION: DOCS RELEVANT---
---GENERATE---
"Output from node 'generate':"
'---'
{ 'messages': [ 'Zero-shot learning involves feeding the task text to the '
                'model and asking for results directly. Few-shot prompting, on '
                'the other hand, involves presenting a few examples in the '
                'prompt to explain the task intent to the model. Zero-shot '
                'generation focuses on finding prompts that can trigger '
                'harmful output based on a preset prompt.']}
'\n---\n'
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Differentiate zero shot and few shot prompting from the blog post'),
                AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"zero shot prompting vs few shot prompting"}', 'name': 'retrieve_blog_posts'}}),
                FunctionMessage(content="Basic Prompting#\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\nZero-Shot#\nZero-shot learning is to simply feed the task text to the model and ask for results.\n(All the sentiment analysis examples are from SST-2)\nText: i'll bet the video game is a lot more fun than the film.\n\nInstruction Prompting#\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\propto \\exp(r(\\mathbf{x}, \\mathbf{y}) / \\tau)$\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.", name='retrieve_blog_posts'),
                'Zero-shot learning involves feeding the task text to the '
                'model and asking for results directly. Few-shot prompting, on '
                'the other hand, involves presenting a few examples in the '
                'prompt to explain the task intent to the model. Zero-shot '
                'generation focuses on finding prompts that can trigger '
                'harmful output based on a preset prompt.']}
'\n---\n'
</code></pre>

<h2 id="conclusions">Conclusions</h2>
<h2 id="references">References</h2>


  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/disqus-comments/">a post with disqus comments</a>
  </li>

<div id="disqus_thread" style="max-width: 1200px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'https-shresthakamal-com-np';
        var disqus_identifier = '/blog/2024/code';
        var disqus_title      = "Creating RAG application using Langgraph";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a>
</noscript>
</div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Kamal  Shrestha (कमल श्रेष्ठ). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: March 21, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-K1NXC136FW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-K1NXC136FW');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
