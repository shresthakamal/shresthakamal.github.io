<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shresthakamal.com.np/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shresthakamal.com.np/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-21T03:41:34+00:00</updated><id>https://shresthakamal.com.np/feed.xml</id><title type="html">blank</title><subtitle>I work in the intersection of ML, DL, NLP and some CV.
</subtitle><entry><title type="html">Creating RAG application using Langgraph</title><link href="https://shresthakamal.com.np/blog/2024/code/" rel="alternate" type="text/html" title="Creating RAG application using Langgraph" /><published>2024-03-01T15:09:00+00:00</published><updated>2024-03-01T15:09:00+00:00</updated><id>https://shresthakamal.com.np/blog/2024/code</id><content type="html" xml:base="https://shresthakamal.com.np/blog/2024/code/"><![CDATA[<h2 id="prerequisites">Prerequisites</h2>
<ol>
  <li>Good Knowledge on langchain, creating agents used for multiple tools selections</li>
  <li>Well-versed in retrieval-augmented generations, vector databases, and text embeddings</li>
</ol>

<h2 id="learning-objectives">Learning Objectives</h2>
<p>By the end of this post, all readers will be able to:</p>
<ol>
  <li>Understand the difference between langchain agents with multiple tools selections and langgraphs</li>
  <li>Create a langgraph agentic rag using stateful nodes capable of making logical decisions using LLMs</li>
</ol>

<h2 id="langchain-vs-langgraph">Langchain Vs Langgraph</h2>

<p style="text-align:justify;">
<img src="/assets/blogs/langgraph-agentic-rag/langgraph.webp" style="float: right;padding:20px" />

Langchain is constructed upon the fundamental principle of a sequential acyclic flow of execution, spanning from user inquiry to the generation of results by the Language Model (LLM). 
<br />
In RAG (Retrieval-Augmented Generation), the procedural flow within the application starts with the user query, progresses through stages such as database extraction, tool selection, context enrichment, and ultimately finishes at result generation. Furthermore, this process may also encompass evaluation, query transformations, and similar steps. The essence of Langchain lies in its utilization of interconnected "chains," wherein each element is systematically linked in a sequential fashion. 
<br /><br />
It's important to understand that the decision-making process in this context operates sequentially. When choices are made, they set the course for subsequent actions, creating a flow of information. What's significant about this approach is the absence of a feedback loop or the opportunity for re-evaluation based on outcomes. Among its various capabilities, langgraph is specifically designed to introduce a cyclic, stateful, multi-actor application framework using LLMs. Previously, Langchain agents could only re-execute a single action using CoT prompts like ReAct. They could assess the outcome, observe results, and potentially adjust course. 
<br /><br />
However, the ability to reprocess multiple steps of execution was unavailable until the introduction of langgraph. With langgraph, we now have the capability to re-execute multiple elements in a feedback loop fashion, enabling a more iterative approach to decision-making and action.
<br />
Agents developed within langgraph will possess the capability to make decisions, assess their effectiveness, and if necessary, rerun processes to enhance output. This functionality extends the LangChain Expression Language by facilitating the coordination of multiple chains or actors across various stages of computation in a cyclic fashion, as stated by LangChain. With langgraph, you now have the flexibility to outline an application resembling a flowchart, with each decision node rooted in the logical determinations of LLMs. This enables a structured and iterative approach to application development, allowing for refinement and optimization at every step.
<br /><br /><br />

<b>Now, in this blog post, we will be creating a RAG agent using langgraph by designing a sequence of execution nodes and the overall flow of the application.</b>
</p>

<p><br /></p>

<h2 id="rag-using-agents-in-langgraph">RAG using agents in langgraph</h2>

<p><br /></p>

<h3 id="installations">Installations</h3>
<p>The first and foremost thing to do is to make sure all the necessary libraries are installed. Make sure you have the upgraded version of langchain because a lot of modules are segregated out of langchain to openai, community and hub.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_community</span> <span class="n">langchain</span><span class="o">-</span><span class="n">openai</span> <span class="n">langchainhub</span> <span class="n">chromadb</span> <span class="n">langchain</span> <span class="n">langgraph</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="load-openai-api-keys">Load OPENAI API keys</h3>
<p>We will use the python-dotenv package to load our api keys.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="k">import</span> <span class="n">os</span>

    <span class="n">from</span> <span class="n">dotenv</span> <span class="k">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>

    <span class="cp"># read local .env file
</span>    <span class="n">_</span> <span class="o">=</span> <span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="indexing">Indexing</h3>

<p>The initial step in any machine learning projects is to collect the data and conduct pre processing for further usage.</p>

<ol>
  <li>In this step here, we will be loading our data from three blog posts from Lilian Weng using the <code class="language-plaintext highlighter-rouge">WebBaseLoader</code> from langchain that loads the data in the form of txt files in a list.</li>
  <li>Once each links are processed in a list of <code class="language-plaintext highlighter-rouge">Document</code> objects in langchain, we will start creating the chunks.</li>
  <li>We will use <code class="language-plaintext highlighter-rouge">RecursiveCharacterTextSplitter</code> with <code class="language-plaintext highlighter-rouge">chunk_size = 100</code> and <code class="language-plaintext highlighter-rouge">chunk_overlap = 50</code> to create document chunks</li>
  <li>Creating a vector store using <code class="language-plaintext highlighter-rouge">chromadb</code>.</li>
</ol>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td class="code"><pre>    <span class="cp"># necessary imports
</span>    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">text_splitter</span> <span class="k">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
    <span class="n">from</span> <span class="n">langchain_community</span><span class="p">.</span><span class="n">document_loaders</span> <span class="k">import</span> <span class="n">WebBaseLoader</span>
    <span class="n">from</span> <span class="n">langchain_community</span><span class="p">.</span><span class="n">vectorstores</span> <span class="k">import</span> <span class="n">Chroma</span>
    <span class="n">from</span> <span class="n">langchain_openai</span> <span class="k">import</span> <span class="n">OpenAIEmbeddings</span>

    <span class="cp"># data sources
</span>    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-06-23-agent/"</span><span class="p">,</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"</span><span class="p">,</span>
        <span class="s">"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="cp"># get the documents in txt based formats
</span>    <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">).</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="n">in</span> <span class="n">urls</span><span class="p">]</span>
    <span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="n">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="n">in</span> <span class="n">sublist</span><span class="p">]</span>

    <span class="cp"># split the documents into chunks
</span>    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span>
    <span class="p">)</span>
    <span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>
    
    <span class="cp"># create a chromadb based on the chunks above
</span>    <span class="cp"># each chunks are embedded using OPENAI embeddings and the database is named as "rag-chroma"
</span>    <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="n">from_documents</span><span class="p">(</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s">"rag-chroma"</span><span class="p">,</span>
        <span class="n">embedding</span><span class="o">=</span><span class="n">OpenAIEmbeddings</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="cp"># instantiate a vector store backed retriever
</span>    <span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="n">as_retriever</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>We will now create an agent tool that the agent can access while retreiving documents from the chroma database. This is very similar to what we do in LCEL / langchain while creating a tool and a tool executor.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">tools</span><span class="p">.</span><span class="n">retriever</span> <span class="k">import</span> <span class="n">create_retriever_tool</span>

    <span class="cp"># Tool Definition with the name of the tool and its description
</span>    <span class="n">tool</span> <span class="o">=</span> <span class="n">create_retriever_tool</span><span class="p">(</span>
        <span class="n">retriever</span><span class="p">,</span>
        <span class="s">"retrieve_blog_posts"</span><span class="p">,</span>
        <span class="s">"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs."</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>

    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">prebuilt</span> <span class="k">import</span> <span class="n">ToolExecutor</span>
    <span class="n">tool_executor</span> <span class="o">=</span> <span class="n">ToolExecutor</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
    <span class="cp"># tool_executor that can be called with agent</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="states-in-a-langgraph">States in a langgraph</h3>

<p>A state in a langgraph is very similar to attributes in a class object. Whenever a flow execution moves from one node to another node, the output of nodes are written in the states object (overridden or added). Overridden means change in state whereas  addition means collection of states. For example: chat messages will be added where as grades of documents will be overidden.
<br /></p>

<p>You can think of states as sessions values as well like in streamlit (if you have used streamlit) or cache.
<br /></p>

<p>In the following example, we define a <code class="language-plaintext highlighter-rouge">AgentState</code> class that collects all the messages generated in the execution. It is nothing but a dictionary with key, value pairs.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="k">import</span> <span class="k">operator</span>
    <span class="n">from</span> <span class="n">typing</span> <span class="k">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">TypedDict</span>

    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">BaseMessage</span>


    <span class="k">class</span> <span class="nc">AgentState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">)</span><span class="o">:</span>
        <span class="n">messages</span><span class="o">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="k">operator</span><span class="p">.</span><span class="n">add</span><span class="p">]</span>

    <span class="cp"># Here, the opetor.add is defined so that we can simply pass the Sequence[BaseMessage] as parameters
</span>    <span class="cp"># and no need to define add functions. like AgentState([k, v])</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="nodes-of-execution">Nodes of execution</h3>
<p><img src="/assets/blogs/langgraph-agentic-rag/state-flow.png" width="100%" align="center" /></p>

<h4 id="imports">imports</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="n">from</span> <span class="n">langchain</span> <span class="k">import</span> <span class="n">hub</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">output_parsers</span> <span class="k">import</span> <span class="n">PydanticOutputParser</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">prompts</span> <span class="k">import</span> <span class="n">PromptTemplate</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">tools</span><span class="p">.</span><span class="n">render</span> <span class="k">import</span> <span class="n">format_tool_to_openai_function</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">function_calling</span> <span class="k">import</span> <span class="n">convert_to_openai_tool</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">BaseMessage</span><span class="p">,</span> <span class="n">FunctionMessage</span>
    <span class="n">from</span> <span class="n">langchain</span><span class="p">.</span><span class="n">output_parsers</span><span class="p">.</span><span class="n">openai_tools</span> <span class="k">import</span> <span class="n">PydanticToolsParser</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">pydantic_v1</span> <span class="k">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
    <span class="n">from</span> <span class="n">langchain_openai</span> <span class="k">import</span> <span class="n">ChatOpenAI</span>
    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">prebuilt</span> <span class="k">import</span> <span class="n">ToolInvocation</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">output_parsers</span> <span class="k">import</span> <span class="n">StrOutputParser</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="agent-node">agent node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">agent</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Invokes the agent model to generate a response based on the current state. Given</span><span class="err">
</span><span class="s">        the question, it will decide to retrieve using the retriever tool, or simply end.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with the agent response apended to messages</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---CALL AGENT---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">format_tool_to_openai_function</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="n">in</span> <span class="n">tools</span><span class="p">]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">bind_functions</span><span class="p">(</span><span class="n">functions</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="cp"># We return a list, because this will get added to the existing list
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="retreive-node">retreive node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Uses tool to execute retrieval.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with retrieved docs</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---EXECUTE RETRIEVAL---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="cp"># Based on the continue condition
</span>        <span class="cp"># we know the last message involves a function call
</span>        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="cp"># We construct an ToolInvocation from the function_call
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">ToolInvocation</span><span class="p">(</span>
            <span class="n">tool</span><span class="o">=</span><span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s">"function_call"</span><span class="p">][</span><span class="s">"name"</span><span class="p">],</span>
            <span class="n">tool_input</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span>
                <span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s">"function_call"</span><span class="p">][</span><span class="s">"arguments"</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="cp"># We call the tool_executor and get back a response
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">tool_executor</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">function_message</span> <span class="o">=</span> <span class="n">FunctionMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">str</span><span class="p">(</span><span class="n">response</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">action</span><span class="p">.</span><span class="n">tool</span><span class="p">)</span>

        <span class="cp"># We return a list, because this will get added to the existing list
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">function_message</span><span class="p">]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="should_retreive-node">should_retreive node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">should_retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Decides whether the agent should retrieve more information or end the process.</span><span class="err">

</span><span class="s">        This function checks the last message in the state for a function call. If a function call is</span><span class="err">
</span><span class="s">        present, the process continues to retrieve information. Otherwise, it ends the process.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            str: A decision to either "</span><span class="k">continue</span><span class="s">" the retrieval process or "</span><span class="n">end</span><span class="s">" it</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---DECIDE TO RETRIEVE---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="cp"># If there is no function call, then we finish
</span>        <span class="k">if</span> <span class="s">"function_call"</span> <span class="n">not</span> <span class="n">in</span> <span class="n">last_message</span><span class="p">.</span><span class="n">additional_kwargs</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DO NOT RETRIEVE / DONE---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"end"</span>
        <span class="cp"># Otherwise there is a function call, so we continue
</span>        <span class="k">else</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: RETRIEVE---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"continue"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="grade_documents-node">grade_documents node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">grade_documents</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Determines whether the retrieved documents are relevant to the question.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            str: A decision for whether the documents are relevant or not</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---CHECK RELEVANCE---"</span><span class="p">)</span>

        <span class="cp"># Data model
</span>        <span class="k">class</span> <span class="nc">grade</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">)</span><span class="o">:</span>
            <span class="s">"""Binary score for relevance check."""</span>

            <span class="n">binary_score</span><span class="o">:</span> <span class="n">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Relevance score 'yes' or 'no'"</span><span class="p">)</span>

        <span class="cp"># LLM
</span>        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="cp"># Tool
</span>        <span class="n">grade_tool_oai</span> <span class="o">=</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">grade</span><span class="p">)</span>

        <span class="cp"># LLM with tool and enforce invocation
</span>        <span class="n">llm_with_tool</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">bind</span><span class="p">(</span>
            <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">grade_tool_oai</span><span class="p">)],</span>
            <span class="n">tool_choice</span><span class="o">=</span><span class="p">{</span><span class="s">"type"</span><span class="o">:</span> <span class="s">"function"</span><span class="p">,</span> <span class="s">"function"</span><span class="o">:</span> <span class="p">{</span><span class="s">"name"</span><span class="o">:</span> <span class="s">"grade"</span><span class="p">}},</span>
        <span class="p">)</span>

        <span class="cp"># Parser
</span>        <span class="n">parser_tool</span> <span class="o">=</span> <span class="n">PydanticToolsParser</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">grade</span><span class="p">])</span>

        <span class="cp"># Prompt
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
            <span class="k">template</span><span class="o">=</span><span class="s">"""You are a grader assessing relevance of a retrieved document to a user question. </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">            Here is the retrieved document: </span><span class="se">\n\n</span><span class="s"> {context} </span><span class="se">\n\n</span><span class="err">
</span><span class="s">            Here is the user question: {question} </span><span class="se">\n</span><span class="err">
</span><span class="s">            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. </span><span class="se">\n</span><span class="err">
</span><span class="s">            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""</span><span class="p">,</span>
            <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s">"context"</span><span class="p">,</span> <span class="s">"question"</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="cp"># Chain
</span>        <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm_with_tool</span> <span class="o">|</span> <span class="n">parser_tool</span>

        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="p">.</span><span class="n">content</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="o">:</span> <span class="n">question</span><span class="p">,</span> <span class="s">"context"</span><span class="o">:</span> <span class="n">docs</span><span class="p">})</span>

        <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">binary_score</span>

        <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s">"yes"</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DOCS RELEVANT---"</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"yes"</span>

        <span class="nl">else:</span>
            <span class="n">print</span><span class="p">(</span><span class="s">"---DECISION: DOCS NOT RELEVANT---"</span><span class="p">)</span>
            <span class="n">print</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">binary_score</span><span class="p">)</span>
            <span class="k">return</span> <span class="s">"no"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="generate-node">generate node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">generate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Generate answer</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with re-phrased question</span><span class="err">
</span><span class="s">        """</span>
        <span class="n">print</span><span class="p">(</span><span class="s">"---GENERATE---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="p">.</span><span class="n">content</span>

        <span class="cp"># Prompt
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="n">pull</span><span class="p">(</span><span class="s">"rlm/rag-prompt"</span><span class="p">)</span>

        <span class="cp"># LLM
</span>        <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="cp"># Post-processing
</span>        <span class="n">def</span> <span class="n">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">:</span>
            <span class="k">return</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="n">in</span> <span class="n">docs</span><span class="p">)</span>

        <span class="cp"># Chain
</span>        <span class="n">rag_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

        <span class="cp"># Run
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"context"</span><span class="o">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s">"question"</span><span class="o">:</span> <span class="n">question</span><span class="p">})</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="rewrite-node">rewrite node</h4>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="code"><pre>    <span class="n">def</span> <span class="n">rewrite</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">:</span>
        <span class="s">"""</span><span class="err">
</span><span class="s">        Transform the query to produce a better question.</span><span class="err">

</span><span class="s">        Args:</span><span class="err">
</span><span class="s">            state (messages): The current state</span><span class="err">

</span><span class="s">        Returns:</span><span class="err">
</span><span class="s">            dict: The updated state with re-phrased question</span><span class="err">
</span><span class="s">        """</span>

        <span class="n">print</span><span class="p">(</span><span class="s">"---TRANSFORM QUERY---"</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">"messages"</span><span class="p">]</span>

        <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">content</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">HumanMessage</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">f</span><span class="s">""" </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">        Look at the input and try to reason about the underlying semantic intent / meaning. </span><span class="se">\n</span><span class="s"> </span><span class="err">
</span><span class="s">        Here is the initial question:</span><span class="err">
</span><span class="s">        </span><span class="se">\n</span><span class="s"> ------- </span><span class="se">\n</span><span class="err">
</span><span class="s">        {question} </span><span class="err">
</span><span class="s">        </span><span class="se">\n</span><span class="s"> ------- </span><span class="se">\n</span><span class="err">
</span><span class="s">        Formulate an improved question: """</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="cp"># Grader
</span>        <span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="graph--workflow">Graph / Workflow</h3>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td><td class="code"><pre>    <span class="n">from</span> <span class="n">langgraph</span><span class="p">.</span><span class="n">graph</span> <span class="k">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span>

    <span class="cp"># Define a new graph
</span>    <span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">AgentState</span><span class="p">)</span>

    <span class="cp"># Define the nodes we will cycle between
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"agent"</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>  <span class="err">#</span> <span class="n">agent</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"retrieve"</span><span class="p">,</span> <span class="n">retrieve</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"rewrite"</span><span class="p">,</span> <span class="n">rewrite</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_node</span><span class="p">(</span><span class="s">"generate"</span><span class="p">,</span> <span class="n">generate</span><span class="p">)</span>  <span class="err">#</span> <span class="n">retrieval</span>


    <span class="cp"># Call agent node to decide to retrieve or not
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">set_entry_point</span><span class="p">(</span><span class="s">"agent"</span><span class="p">)</span>

    <span class="cp"># Decide whether to retrieve
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
        <span class="s">"agent"</span><span class="p">,</span>
        <span class="cp"># Assess agent decision
</span>        <span class="n">should_retrieve</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="cp"># Call tool node
</span>            <span class="s">"continue"</span><span class="o">:</span> <span class="s">"retrieve"</span><span class="p">,</span>
            <span class="s">"end"</span><span class="o">:</span> <span class="n">END</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>

    <span class="cp"># Edges taken after the `action` node is called.
</span>    <span class="n">workflow</span><span class="p">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
        <span class="s">"retrieve"</span><span class="p">,</span>
        <span class="cp"># Assess agent decision
</span>        <span class="n">grade_documents</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="s">"yes"</span><span class="o">:</span> <span class="s">"generate"</span><span class="p">,</span>
            <span class="s">"no"</span><span class="o">:</span> <span class="s">"rewrite"</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s">"generate"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
    <span class="n">workflow</span><span class="p">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s">"rewrite"</span><span class="p">,</span> <span class="s">"agent"</span><span class="p">)</span>

    <span class="cp"># Compile
</span>    <span class="n">app</span> <span class="o">=</span> <span class="n">workflow</span><span class="p">.</span><span class="n">compile</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="user-input-and-generation">User Input and Generation</h3>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="k">import</span> <span class="n">pprint</span>
    <span class="n">from</span> <span class="n">langchain_core</span><span class="p">.</span><span class="n">messages</span> <span class="k">import</span> <span class="n">HumanMessage</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Tell me about langgraph"</span><span class="p">)]}</span>
    <span class="k">for</span> <span class="n">output</span> <span class="n">in</span> <span class="n">app</span><span class="p">.</span><span class="n">stream</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="n">in</span> <span class="n">output</span><span class="p">.</span><span class="n">items</span><span class="p">()</span><span class="o">:</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">f</span><span class="s">"Output from node '{key}':"</span><span class="p">)</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="s">"---"</span><span class="p">)</span>
            <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">None</span><span class="p">)</span>
        <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">---</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"langgraph"}', 'name': 'retrieve_blog_posts'}})]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: RETRIEVE---
---EXECUTE RETRIEVAL---
"Output from node 'retrieve':"
'---'
{ 'messages': [ FunctionMessage(content='$$\n\n$$\n\n$$\n\n$$', name='retrieve_blog_posts')]}
'\n---\n'
---CHECK RELEVANCE---
---DECISION: DOCS NOT RELEVANT---
no
---TRANSFORM QUERY---
"Output from node 'rewrite':"
'---'
{ 'messages': [ AIMessage(content='What is the purpose or function of langgraph?')]}
'\n---\n'
---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='I couldn\'t find specific information about "langgraph" in Lilian Weng\'s blog posts. Could you provide more context or details about langgraph so I can better assist you?')]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: DO NOT RETRIEVE / DONE---
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Tell me about langgraph'),
                AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"langgraph"}', 'name': 'retrieve_blog_posts'}}),
                FunctionMessage(content='$$\n\n$$\n\n$$\n\n$$', name='retrieve_blog_posts'),
                AIMessage(content='What is the purpose or function of langgraph?'),
                AIMessage(content='I couldn\'t find specific information about "langgraph" in Lilian Weng\'s blog posts. Could you provide more context or details about langgraph so I can better assist you?')]}
'\n---\n'
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Differentiate between agents and chain in langchain"</span><span class="p">)]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OUTPUT

---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='In Langchain, an "agent" refers to an entity that interacts with the Langchain network to perform various tasks, such as executing smart contracts, processing transactions, or providing services. Agents in Langchain can be autonomous entities that act on behalf of users or organizations to carry out specific functions within the network.\n\nOn the other hand, a "chain" in Langchain typically refers to the blockchain network itself. It is the decentralized and distributed ledger that stores transaction data, smart contracts, and other information in a secure and immutable manner. The chain in Langchain is responsible for maintaining the integrity of the network, validating transactions, and ensuring consensus among network participants.\n\nIn summary, agents are the entities that interact with the Langchain network to perform tasks, while the chain represents the underlying blockchain infrastructure that facilitates these interactions and maintains the network\'s operations.')]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: DO NOT RETRIEVE / DONE---
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Differentiate between agents and chain in langchain'),
                AIMessage(content='In Langchain, an "agent" refers to an entity that interacts with the Langchain network to perform various tasks, such as executing smart contracts, processing transactions, or providing services. Agents in Langchain can be autonomous entities that act on behalf of users or organizations to carry out specific functions within the network.\n\nOn the other hand, a "chain" in Langchain typically refers to the blockchain network itself. It is the decentralized and distributed ledger that stores transaction data, smart contracts, and other information in a secure and immutable manner. The chain in Langchain is responsible for maintaining the integrity of the network, validating transactions, and ensuring consensus among network participants.\n\nIn summary, agents are the entities that interact with the Langchain network to perform tasks, while the chain represents the underlying blockchain infrastructure that facilitates these interactions and maintains the network\'s operations.')]}
'\n---\n'
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"messages"</span><span class="o">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s">"Differentiate zero shot and few shot prompting from the blog posts"</span><span class="p">)]}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<pre><code class="language-OUTPUT">
---CALL AGENT---
"Output from node 'agent':"
'---'
{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"zero shot prompting vs few shot prompting"}', 'name': 'retrieve_blog_posts'}})]}
'\n---\n'
---DECIDE TO RETRIEVE---
---DECISION: RETRIEVE---
---EXECUTE RETRIEVAL---
"Output from node 'retrieve':"
'---'
{ 'messages': [ FunctionMessage(content="Basic Prompting#\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\nZero-Shot#\nZero-shot learning is to simply feed the task text to the model and ask for results.\n(All the sentiment analysis examples are from SST-2)\nText: i'll bet the video game is a lot more fun than the film.\n\nInstruction Prompting#\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\propto \\exp(r(\\mathbf{x}, \\mathbf{y}) / \\tau)$\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.", name='retrieve_blog_posts')]}
'\n---\n'
---CHECK RELEVANCE---
---DECISION: DOCS RELEVANT---
---GENERATE---
"Output from node 'generate':"
'---'
{ 'messages': [ 'Zero-shot learning involves feeding the task text to the '
                'model and asking for results directly. Few-shot prompting, on '
                'the other hand, involves presenting a few examples in the '
                'prompt to explain the task intent to the model. Zero-shot '
                'generation focuses on finding prompts that can trigger '
                'harmful output based on a preset prompt.']}
'\n---\n'
"Output from node '__end__':"
'---'
{ 'messages': [ HumanMessage(content='Differentiate zero shot and few shot prompting from the blog post'),
                AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"zero shot prompting vs few shot prompting"}', 'name': 'retrieve_blog_posts'}}),
                FunctionMessage(content="Basic Prompting#\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\nZero-Shot#\nZero-shot learning is to simply feed the task text to the model and ask for results.\n(All the sentiment analysis examples are from SST-2)\nText: i'll bet the video game is a lot more fun than the film.\n\nInstruction Prompting#\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.\nStochastic few-shot generation: The red team prompts found from the above step are then used as few-shot examples to generate more similar cases. Each zero-shot test case might be selected in few-shot examples with a probability $\\propto \\exp(r(\\mathbf{x}, \\mathbf{y}) / \\tau)$\n\nZero-shot generation: This is to find a number of prompts that can trigger harmful output conditioned on a preset prompt.", name='retrieve_blog_posts'),
                'Zero-shot learning involves feeding the task text to the '
                'model and asking for results directly. Few-shot prompting, on '
                'the other hand, involves presenting a few examples in the '
                'prompt to explain the task intent to the model. Zero-shot '
                'generation focuses on finding prompts that can trigger '
                'harmful output based on a preset prompt.']}
'\n---\n'
</code></pre>

<h2 id="conclusions">Conclusions</h2>
<h2 id="references">References</h2>]]></content><author><name></name></author><category term="nlp" /><category term="rag" /><category term="nlp" /><category term="langchain" /><category term="llm" /><summary type="html"><![CDATA[stateful langgraph nodes for RAG applications rather than multiple tools selection in LCEL]]></summary></entry><entry><title type="html">a post with disqus comments</title><link href="https://shresthakamal.com.np/blog/2015/disqus-comments/" rel="alternate" type="text/html" title="a post with disqus comments" /><published>2015-10-20T15:59:00+00:00</published><updated>2015-10-20T15:59:00+00:00</updated><id>https://shresthakamal.com.np/blog/2015/disqus-comments</id><content type="html" xml:base="https://shresthakamal.com.np/blog/2015/disqus-comments/"><![CDATA[<p>This post shows how to add DISQUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with disqus comments]]></summary></entry></feed>